"""
AI-Powered Entity Extraction
============================

Uses Claude Agent to extract structured entities from note content:
- Todos and action items
- People mentions (@person)
- Note links ([[note]])
- Tags (#tag)
- Projects and concepts
- Dates and deadlines

The agent can also store its own analysis notes in the agent_ideas/ folder.
"""

import logging
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

# Entity types recognized by the system
ENTITY_TYPES = ['todo', 'person', 'project', 'concept', 'date', 'mention', 'tag', 'link']


class EntityExtractor:
    """
    AI-powered entity extraction from note content.

    Uses Claude Agent for intelligent extraction that can understand
    context, implicit todos, and natural language references.
    """

    def __init__(self, working_dir: Optional[Path] = None, use_ai: bool = True):
        """
        Initialize the entity extractor.

        Args:
            working_dir: Base directory for file operations
            use_ai: Whether to use AI for extraction (vs regex-only fallback)
        """
        self.working_dir = working_dir or Path.cwd()
        self.use_ai = use_ai
        self.agent_ideas_dir = self.working_dir / "agent_ideas"

        # Ensure agent_ideas directory exists
        self._ensure_agent_ideas_dir()

    def _ensure_agent_ideas_dir(self):
        """Create the agent_ideas directory if it doesn't exist."""
        try:
            self.agent_ideas_dir.mkdir(parents=True, exist_ok=True)

            # Create a README if it doesn't exist
            readme_path = self.agent_ideas_dir / "README.md"
            if not readme_path.exists():
                readme_content = """# Agent Ideas

This folder contains notes and analysis generated by the Obby semantic insights agent.

## Contents

- **patterns.md** - Discovered patterns across your notes
- **themes.md** - Recurring themes and topics
- **connections.md** - Hidden connections between notes
- **insights.md** - Raw insights awaiting review

## Note

These files are managed by the AI agent. You can read them for insights
but avoid modifying them directly as they may be regenerated.
"""
                readme_path.write_text(readme_content)
                logger.info(f"Created agent_ideas directory at {self.agent_ideas_dir}")
        except Exception as e:
            logger.warning(f"Could not create agent_ideas directory: {e}")

    async def extract_entities_ai(
        self,
        note_path: str,
        content: str,
        save_analysis: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Extract entities from note content using Claude AI.

        Args:
            note_path: Path to the note file
            content: Note content to analyze
            save_analysis: Whether to save agent analysis to agent_ideas/

        Returns:
            List of extracted entity dictionaries
        """
        try:
            from ai.claude_agent_client import ClaudeAgentClient
            from claude_agent_sdk import query, ClaudeAgentOptions
        except ImportError as e:
            logger.warning(f"Claude SDK not available, falling back to regex: {e}")
            return self.extract_entities_regex(note_path, content)

        entities = []

        # Build extraction prompt
        prompt = self._build_extraction_prompt(note_path, content)

        try:
            system_prompt = """You are an entity extraction assistant for a note-taking application.
Your job is to identify and extract structured entities from note content.

Extract the following entity types:
- todo: Action items, tasks, checkboxes ([ ], - [ ], TODO:, FIXME:)
- person: People mentions (@name or names in context)
- project: Project names or references
- concept: Key ideas or topics
- date: Dates and deadlines
- mention: @mentions
- tag: #hashtags
- link: [[wiki-links]] to other notes

Return your findings as a JSON array of objects with this structure:
{
  "entity_type": "todo|person|project|concept|date|mention|tag|link",
  "entity_value": "the extracted value",
  "context": "surrounding text for reference",
  "line_number": null,
  "status": "active|completed"
}

For todos, mark status as "completed" if the checkbox is checked ([x]).
Be thorough but avoid duplicates. Focus on meaningful entities."""

            options = ClaudeAgentOptions(
                cwd=str(self.working_dir),
                allowed_tools=["Read"],
                max_turns=3,
                system_prompt=system_prompt
            )

            result_text = []
            async for message in query(prompt=prompt, options=options):
                message_type = message.__class__.__name__
                if message_type == "AssistantMessage":
                    if hasattr(message, 'content'):
                        for block in message.content:
                            if hasattr(block, 'text'):
                                result_text.append(block.text)

            # Parse the JSON response
            full_response = "\n".join(result_text)
            entities = self._parse_ai_response(full_response, note_path)

            # Optionally save analysis
            if save_analysis and entities:
                await self._save_analysis(note_path, entities, full_response)

            logger.info(f"AI extracted {len(entities)} entities from {note_path}")
            return entities

        except Exception as e:
            logger.error(f"AI extraction failed for {note_path}: {e}", exc_info=True)
            # Fall back to regex extraction
            return self.extract_entities_regex(note_path, content)

    def _build_extraction_prompt(self, note_path: str, content: str) -> str:
        """Build the extraction prompt for the AI."""
        # Truncate very long content
        max_content_length = 8000
        if len(content) > max_content_length:
            content = content[:max_content_length] + "\n\n[Content truncated...]"

        return f"""Extract all entities from this note.

Note path: {note_path}

Note content:
---
{content}
---

Return ONLY a JSON array of entities. No explanation text, just the JSON array."""

    def _parse_ai_response(self, response: str, note_path: str) -> List[Dict[str, Any]]:
        """Parse the AI response to extract entity JSON."""
        entities = []

        # Try to find JSON array in response
        json_match = re.search(r'\[[\s\S]*\]', response)
        if json_match:
            try:
                raw_entities = json.loads(json_match.group())
                for entity in raw_entities:
                    # Validate and normalize
                    if isinstance(entity, dict) and 'entity_type' in entity and 'entity_value' in entity:
                        entities.append({
                            'note_path': note_path,
                            'entity_type': entity.get('entity_type', 'concept'),
                            'entity_value': entity.get('entity_value', ''),
                            'context': entity.get('context', ''),
                            'status': entity.get('status', 'active'),
                            'line_number': entity.get('line_number'),
                            'extracted_at': datetime.now().isoformat()
                        })
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse AI response as JSON: {e}")

        return entities

    async def _save_analysis(
        self,
        note_path: str,
        entities: List[Dict[str, Any]],
        raw_response: str
    ):
        """Save agent analysis to agent_ideas directory."""
        try:
            # Append to insights log
            insights_file = self.agent_ideas_dir / "insights.md"

            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
            entry = f"\n## {timestamp} - {note_path}\n\n"

            # Summarize entities
            by_type = {}
            for e in entities:
                t = e.get('entity_type', 'unknown')
                by_type.setdefault(t, []).append(e.get('entity_value', ''))

            for entity_type, values in by_type.items():
                entry += f"- **{entity_type}s**: {', '.join(values[:5])}"
                if len(values) > 5:
                    entry += f" (+{len(values) - 5} more)"
                entry += "\n"

            entry += "\n---\n"

            # Append to file
            with open(insights_file, 'a', encoding='utf-8') as f:
                f.write(entry)

        except Exception as e:
            logger.warning(f"Failed to save analysis: {e}")

    def extract_entities_regex(self, note_path: str, content: str) -> List[Dict[str, Any]]:
        """
        Extract entities using regex patterns (fallback when AI unavailable).

        Args:
            note_path: Path to the note file
            content: Note content to analyze

        Returns:
            List of extracted entity dictionaries
        """
        entities = []
        lines = content.split('\n')

        for line_num, line in enumerate(lines, 1):
            # Extract unchecked todos: - [ ], * [ ], [ ]
            todo_patterns = [
                r'[-*]\s*\[\s*\]\s*(.+)',  # - [ ] task
                r'^\s*\[\s*\]\s*(.+)',      # [ ] task
                r'TODO:?\s*(.+)',            # TODO: task
                r'FIXME:?\s*(.+)',           # FIXME: task
            ]
            for pattern in todo_patterns:
                matches = re.findall(pattern, line, re.IGNORECASE)
                for match in matches:
                    entities.append({
                        'note_path': note_path,
                        'entity_type': 'todo',
                        'entity_value': match.strip(),
                        'context': line.strip(),
                        'status': 'active',
                        'line_number': line_num,
                        'extracted_at': datetime.now().isoformat()
                    })

            # Extract completed todos: - [x], * [x], [x]
            completed_patterns = [
                r'[-*]\s*\[[xX]\]\s*(.+)',
                r'^\s*\[[xX]\]\s*(.+)',
            ]
            for pattern in completed_patterns:
                matches = re.findall(pattern, line)
                for match in matches:
                    entities.append({
                        'note_path': note_path,
                        'entity_type': 'todo',
                        'entity_value': match.strip(),
                        'context': line.strip(),
                        'status': 'completed',
                        'line_number': line_num,
                        'extracted_at': datetime.now().isoformat()
                    })

            # Extract @mentions
            mentions = re.findall(r'@(\w+)', line)
            for mention in mentions:
                entities.append({
                    'note_path': note_path,
                    'entity_type': 'mention',
                    'entity_value': mention,
                    'context': line.strip(),
                    'status': 'active',
                    'line_number': line_num,
                    'extracted_at': datetime.now().isoformat()
                })

            # Extract #tags
            tags = re.findall(r'#([\w-]+)', line)
            for tag in tags:
                entities.append({
                    'note_path': note_path,
                    'entity_type': 'tag',
                    'entity_value': tag,
                    'context': line.strip(),
                    'status': 'active',
                    'line_number': line_num,
                    'extracted_at': datetime.now().isoformat()
                })

            # Extract [[wiki-links]]
            links = re.findall(r'\[\[([^\]]+)\]\]', line)
            for link in links:
                entities.append({
                    'note_path': note_path,
                    'entity_type': 'link',
                    'entity_value': link,
                    'context': line.strip(),
                    'status': 'active',
                    'line_number': line_num,
                    'extracted_at': datetime.now().isoformat()
                })

            # Extract dates (basic patterns)
            date_patterns = [
                r'\b(\d{4}-\d{2}-\d{2})\b',  # 2024-01-15
                r'\b(\d{1,2}/\d{1,2}/\d{2,4})\b',  # 1/15/24 or 01/15/2024
            ]
            for pattern in date_patterns:
                dates = re.findall(pattern, line)
                for date in dates:
                    entities.append({
                        'note_path': note_path,
                        'entity_type': 'date',
                        'entity_value': date,
                        'context': line.strip(),
                        'status': 'active',
                        'line_number': line_num,
                        'extracted_at': datetime.now().isoformat()
                    })

        logger.info(f"Regex extracted {len(entities)} entities from {note_path}")
        return entities

    async def extract_entities(
        self,
        note_path: str,
        content: str,
        force_ai: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Main entry point for entity extraction.

        Uses AI if available and enabled, otherwise falls back to regex.

        Args:
            note_path: Path to the note file
            content: Note content to analyze
            force_ai: Force AI extraction even if disabled

        Returns:
            List of extracted entity dictionaries
        """
        if self.use_ai or force_ai:
            return await self.extract_entities_ai(note_path, content)
        else:
            return self.extract_entities_regex(note_path, content)
